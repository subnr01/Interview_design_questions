Sort data across multiple machines


https://www.quora.com/Which-algorithms-can-sort-data-that-is-split-across-multiple-machines


Design a trending algorithm for twitter

http://blog.gainlo.co/index.php/2016/05/03/how-to-design-a-trending-algorithm-for-twitter/

Design a news feed function

http://blog.gainlo.co/index.php/2016/03/29/design-news-feed-system-part-1-system-design-interview-questions/

Design a Recommendation function

http://blog.gainlo.co/index.php/2016/05/24/design-a-recommendation-system/

Facebook chat function

http://blog.gainlo.co/index.php/2016/04/19/design-facebook-chat-function/


===========
Random ID generator
http://blog.gainlo.co/index.php/2016/06/07/random-id-generator/



Suppose you are building a social network product like Twitter, you need to store each user in your database with a user ID, which is unique and used to identify each user in the system.

Usually, there are few requirements for ID generators:
* They cannot be arbitrarily long: ( within 64 bits)
* ID is incremented by date (ordering by register date)

There can be some other requirements, especially when you want to scale the system to support millions or even billions of users.

In the simplest case, we can just keep incrementing ID from 1, 2, 3 … N, which in fact is one of the most popular ways to generate ID in many real life projects. If user A’s ID is larger than user B, then we know that A registered later.
However, this approach is hard to scale. Let’s say after one year there are too many users everyday that we have to scale the database to multiple instances. You’ll see that this approach won’t work because it may generate duplicate IDs for different users.

3rd party service
There can be 3rd party service that hands out ids. 
However, the downside of this solution is obvious. Suppose the product is so popular that there can be a huge number of people registering within a single second, the 3rd party server will soon become the bottleneck. The server may either block registration or just crash.

Multiple Macine solution
Thus, we have to scale the ID generation to multiple servers
1. We assign a server ID to each ID generation server and the final ID is a combination of timestamp and the server ID.
2. We can also allow multiple requests within a single timestamp on a single server. We can keep a counter on each server, which indicates how many IDs have been generated in the current timestamp. So the final ID is a combination of timestamp, serverID and the counter.

As mentioned previously, the ID cannot be arbitrarily long so that the counter may end up with only 8 bits for instance. In this case, the server can handle 256 requests within a single timestamp at most. If it frequently exceeds this limit, we need to add more instances.

We ignored a crucial problem in the above analysis. In fact, there’s a hidden assumption that all ID generation servers have the same clock to generate the timestamp, which might not be true in distributed systems.
In reality, system clocks can drastically skew in distributed systems, which can cause our ID generators provide duplicate IDs or IDs with incorrect order. Clock synchronization is out of the scope of this discussion, however, it’s important for you to know such issue in this system. There are quite a few ways to solve this issue, check NTP if you want to know more about it.







========
Design a key value store

http://blog.gainlo.co/index.php/2016/06/14/design-a-key-value-store-part-i/

A key value store can be a simple hash table, while it can be a distributed storage system like Cassandra.

Basic key value storage

The basic design is to use a hash table to store key-value pairs. A hash table allows you to read/write a key-value pair in constant time and it’s extremely easy to use. Most languages have built-in support for this.

However the downside is that you have to keep everything in memory, which may not be possible if the data set is big. There are two solutions for that:
1. Compress the data, use different data representations like bit array.
2. Storing the data in disk: While it is impossible to fit everything in memory, you may store part of the data into disk. Frequently visited data is kept in memory and the rest is on a disk.


Distributed key value storage
The most interesting topic is definitely scaling the key-value storage into multiple machines.

Since a single machine doesn’t have enough storage for all the data, the general idea here is to split the data into multiple machines by some rules and a coordinator machine can direct clients to the machine with requested resource. The question is how to split the data into multiple machines and more importantly, what is a good strategy to partition data?

Sharding 
Suppose all the keys are URLs like http://gainlo.co and we have 26 machines. One approach is to divide all keys (URLs) based on the first character of the URL (after “www”) to these 26 machines. For example, http://gainlo.co will be stored at machine G and http://blog.gainlo.co will be stored at machine B. So what are disadvantages of this design?

A good sharding algorithm should be able to balance traffic equally to all machines. In other words, each machine should receive equal requests ideally. Apparently, the above design doesn’t work well. First of all, the storage is not distributed equally. Probably there are much more URLs starting with “a” than “z”. Secondly, some URLs are much more popular like Facebook and Google.

In order to balance the traffic, you’d better make sure that keys are distributed randomly. Another solution is to use the hash of URL, which usually have much better performance. To design a good sharding algorithm, you should fully understand the application and can estimate the bottleneck of the system.


Part 2
In our previous post, we mostly focus on the basic concepts of key-value store, especially the single machine scenario. When it comes to scaling issues, we need to distribute all the data into multiple machines by some rules and a coordinator machine can direct clients to the machine with requested resource.

There are many things you need to consider when designing the distributed system. When splitting data into multiple machines, it’s important to balance the traffic. That’s why it’s better to make sure that keys are distributed randomly.

System availability
Suppose one of our systems crash, then what happens to key-value storage system. How to address this issue.
The most common solution is replica. By setting machines with duplicate resources, we can significantly reduce the system downtime. If a single machine has 10% of chance to crash every month, then with a single backup machine, we reduce the probability to 1% when both are down.


Replica vs sharding
What is the difference between the two. Do I need to be concerned.


Consistency
Let’s say for machine A1, we have replica A2. How do you make sure that A1 and A2 have the same data? For instance, when inserting a new entry, we need to update both machines. But it’s possible that the write operation fails in one of them. So over time, A1 and A2 might have quite a lot inconsistent data, which is a big problem

We can use the approach of a commit log. Basically, for each node machine, it’ll keep the commit log for each operation, which is like the history of all updates. So when we want to update an entry in machine A, it will first store this request in commit log. And then a separate program will process all the commit logs in order (in a queue). Whenever an operation fails, we can easily recover as we can lookup the commit log

The last approach I’d like to introduce is to resolve conflict in read. Suppose when the requested resource locates in A1, A2 and A3, the coordinator can ask from all three machines. If by any chance the data is different, the system can 
resolve the conflict on the fly.


Read throughput
 Usually, key-value storage system should be able to support a large amount of read requests. So what approaches will you use to improve read throughput?

To improve read throughput, the common approach is always taking advantage of memory. If the data is stored in disk inside each node machine, we can move part of them in memory. A more general idea is to use cache




===========
Design twitter
http://blog.gainlo.co/index.php/2016/02/17/system-design-interview-question-how-to-design-twitter-part-1/

==============

Design youtube

http://blog.gainlo.co/index.php/2016/10/22/design-youtube-part/
==============

Design Hit counter

http://blog.gainlo.co/index.php/2016/09/12/dropbox-interview-design-hit-counter/


=============

Design Ecommerce website

http://blog.gainlo.co/index.php/2016/08/22/design-ecommerce-website-part/


==========
Build a web crawler

http://blog.gainlo.co/index.php/2016/06/29/build-web-crawler/


============
Design a garbage collection system

http://blog.gainlo.co/index.php/2016/07/25/design-a-garbage-collection-system-part-i/


============

Useful links


https://www.hiredintech.com/system-design/the-system-design-process/

https://github.com/checkcheckzz/system-design-interview

https://github.com/shashank88/system_design

mmcgrana/services-engineering


http://www.puncsky.com/blog/2016/02/14/crack-the-system-design-interview/

Distributed systems reading:http://dancres.github.io/Pages/


System design cheatsheet
https://gist.github.com/vasanthk/485d1c25737e8e72759f

=====================
